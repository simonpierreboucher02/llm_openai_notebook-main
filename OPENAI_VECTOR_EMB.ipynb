{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f4d817-25e6-46b5-9cc1-9ef891489035",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #1A73E8;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>OPENAI - VECTOR EMBEDDING RAG</strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #66BB6A;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>Simon-Pierre Boucher</strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #FFA726;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>2024-09-14</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c247a9b9-22af-42f2-b3a8-e65941dfdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05b7191-fcdf-46ba-9f5a-ea43e61a259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_split_book(file_path, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Read a book from a text file and split it into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the text file containing the book.\n",
    "    - chunk_size (int): The number of characters per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list of str): A list of chunks from the book.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Split the text into fixed-size chunks\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda09ad3-2f18-43de-9466-749a15145497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def get_embeddings(texts, api_key):\n",
    "    \"\"\"\n",
    "    Get embeddings for a list of texts using OpenAI's embedding model.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list of str): The list of texts to get embeddings for.\n",
    "    - api_key (str): The API key for OpenAI.\n",
    "\n",
    "    Returns:\n",
    "    - embeddings (list of list of float): A list of embeddings.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"text-embedding-ada-002\",\n",
    "        \"input\": texts\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        embeddings = [item['embedding'] for item in result['data']]\n",
    "        return embeddings\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8997371d-23c8-42a3-a0e2-facca8b3f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, file_path, api_key, max_chunks=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from a book file based on a query using semantic search.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - file_path (str): The path to the text file containing the book.\n",
    "    - api_key (str): The API key for OpenAI.\n",
    "    - max_chunks (int): Maximum number of chunks to return based on relevance.\n",
    "\n",
    "    Returns:\n",
    "    - relevant_chunks (list of str): A list of top relevant chunks sorted by similarity.\n",
    "    \"\"\"\n",
    "    # Step 1: Read and split the book into chunks\n",
    "    chunks = read_and_split_book(file_path)\n",
    "    \n",
    "    # If chunks are empty, return empty list\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Get embeddings for the query and the chunks\n",
    "    query_embedding = get_embeddings([query], api_key)[0]\n",
    "    chunks_embeddings = get_embeddings(chunks, api_key)\n",
    "\n",
    "    # Step 3: Calculate cosine similarity for each chunk in the book\n",
    "    similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunks_embeddings]\n",
    "\n",
    "    # Step 4: Sort chunks by similarity in descending order\n",
    "    sorted_chunks = sorted(zip(chunks, similarities), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Step 5: Select top N relevant chunks\n",
    "    relevant_chunks = [chunk for chunk, sim in sorted_chunks[:max_chunks]]\n",
    "    \n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae6dbea-baf0-4548-96a5-86ddd0253195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, file_path, api_key, max_chunks=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from a book file based on a query using semantic search.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - file_path (str): The path to the text file containing the book.\n",
    "    - api_key (str): The API key for OpenAI.\n",
    "    - max_chunks (int): Maximum number of chunks to return based on relevance.\n",
    "\n",
    "    Returns:\n",
    "    - relevant_chunks (list of str): A list of top relevant chunks sorted by similarity.\n",
    "    \"\"\"\n",
    "    # Step 1: Read and split the book into chunks\n",
    "    chunks = read_and_split_book(file_path)\n",
    "    \n",
    "    # If chunks are empty, return empty list\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Get embeddings for the query and the chunks\n",
    "    query_embedding = get_embeddings([query], api_key)[0]\n",
    "    chunks_embeddings = get_embeddings(chunks, api_key)\n",
    "\n",
    "    # Step 3: Calculate cosine similarity for each chunk in the book\n",
    "    similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunks_embeddings]\n",
    "\n",
    "    # Step 4: Sort chunks by similarity in descending order\n",
    "    sorted_chunks = sorted(zip(chunks, similarities), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Step 5: Select top N relevant chunks\n",
    "    relevant_chunks = [chunk for chunk, sim in sorted_chunks[:max_chunks]]\n",
    "    \n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11678d68-4365-4389-a14e-d67261d4a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_context(context, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    Truncate the context to fit within the max token limit for OpenAI's API.\n",
    "\n",
    "    Parameters:\n",
    "    - context (str): The context string to be truncated.\n",
    "    - max_tokens (int): The maximum number of tokens allowed.\n",
    "\n",
    "    Returns:\n",
    "    - truncated_context (str): The truncated context string.\n",
    "    \"\"\"\n",
    "    # Convert the context into tokens (approximation: 1 token ~ 4 characters in English)\n",
    "    max_characters = max_tokens * 4\n",
    "    \n",
    "    # Truncate the context if it's longer than the max allowed characters\n",
    "    if len(context) > max_characters:\n",
    "        print(f\"Truncating context to {max_characters} characters.\")\n",
    "        return context[:max_characters]\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa268b6-8a1d-4643-9a4b-98411923e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai_response(api_key, model, query, context):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI's API based on the query and context.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key (str): The API key for OpenAI.\n",
    "    - model (str): The model to use for text generation.\n",
    "    - query (str): The user's query.\n",
    "    - context (str): The retrieved context information.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that provides detailed and accurate responses based on given context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"top_p\": 1.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"presence_penalty\": 0.0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error in generating response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcccf734-b223-45e4-9a9f-63a172175d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='rag_system.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def rag_system(api_key, model, query, file_path):\n",
    "    \"\"\"\n",
    "    Perform Retrieval-Augmented Generation (RAG) using OpenAI's API.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key (str): The API key for OpenAI.\n",
    "    - model (str): The model to use for text generation.\n",
    "    - query (str): The user's query.\n",
    "    - file_path (str): The path to the text file containing the book.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from OpenAI based on retrieved chunks.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks using semantic search\n",
    "    retrieved_chunks = retrieve_documents(query, file_path, api_key)\n",
    "    \n",
    "    # Step 2: Handle no relevant chunks found\n",
    "    if not retrieved_chunks:\n",
    "        context = \"No relevant information was found in the book.\"\n",
    "    else:\n",
    "        # Combine retrieved chunks into a single context string\n",
    "        context = \" \".join(retrieved_chunks)\n",
    "\n",
    "    # Step 3: Log the context for debugging (optional)\n",
    "    logging.debug(\"Generated Context: %s\", context)\n",
    "    logging.debug(\"Context length (in characters): %d\", len(context))\n",
    "\n",
    "    # Step 4: Truncate context if necessary\n",
    "    context = truncate_context(context, max_tokens=2048)\n",
    "\n",
    "    # Step 5: Generate response based on context and query\n",
    "    response = generate_openai_response(api_key, model, query, context)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0850052-1a08-4f19-bc71-c8baca1a54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le père pauvre, dans ce contexte, est un personnage qui représente une mentalité financière traditionnelle, souvent associée à la sécurité de l'emploi et à la valeur de l'éducation formelle. Il est décrit comme étant très instruit, mais malgré cela, il demeure financièrement pauvre. L'auteur souligne que son père pauvre avait une vision pessimiste sur la richesse, affirmant des choses comme « Je ne serai jamais riche », ce qui reflète une mentalité de limitation et une prophétie auto-réalisation.\n",
      "\n",
      "L'auteur mentionne cela pour contraster les philosophies de son père pauvre et de son père riche. Le père riche, au contraire, avait une mentalité positive et proactive envers la richesse, se définissant lui-même comme un homme riche même dans les moments difficiles. Ce contraste sert à illustrer l'importance de la mentalité et des croyances personnelles dans la création de richesse et la réussite financière. L'idée est que les attitudes et les croyances que l'on entretient à propos de l'argent peuvent influencer de manière significative les résultats financiers d'une personne.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the RAG system with a text file as the corpus\n",
    "\n",
    "# Path to the text file containing the book\n",
    "file_path = \"corpus.txt\"  # Replace with your actual file path\n",
    "\n",
    "# User query\n",
    "query = \"qui est le père pauvre et pourquoi auteur dit cela\"\n",
    "\n",
    "# OpenAI API key and model\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "# Generate response using the RAG system with chunk-based retrieval\n",
    "response = rag_system(api_key, model, query, file_path)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6087620-f4c3-44d9-91c5-5a00f585097a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
