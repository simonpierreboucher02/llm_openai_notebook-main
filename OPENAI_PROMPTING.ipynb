{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a233bd3-23de-4e49-ae88-a51a42406997",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #1A73E8;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>OPENAI-PROMPTING</strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #66BB6A;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>Simon-Pierre Boucher</strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "    display: block;\n",
    "    padding: 12px 20px;\n",
    "    background-color: #FFA726;\n",
    "    color: white;\n",
    "    border-radius: 30px;\n",
    "    font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "    font-weight: 600;\n",
    "    margin: 15px auto;\n",
    "    width: fit-content;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    letter-spacing: 0.5px;\n",
    "\">\n",
    "    <strong>2024-09-14</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad5b47-f023-441f-8fd2-10e4a062fe9e",
   "metadata": {},
   "source": [
    "## Basic prompt example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5b4bd1-9b92-4056-bc0d-f25768e287f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Assistant:**\n",
      "\n",
      "blue and clear, with fluffy white clouds scattered across its expanse. The sun is shining brightly, casting a warm glow over everything below. It's a perfect day to be outside, enjoying the beauty of nature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params():\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire contenant les paramètres par défaut.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",  # Définir le modèle par défaut\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"top_p\": 1.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"presence_penalty\": 0.0\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=2000, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_openai_response(response):\n",
    "    \"\"\"\n",
    "    Formate la réponse de l'API OpenAI pour afficher uniquement le message de l'assistant.\n",
    "    \"\"\"\n",
    "    if response and \"choices\" in response:\n",
    "        assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        formatted_text = f\"**Assistant:**\\n\\n{assistant_message}\\n\"\n",
    "        return formatted_text\n",
    "    else:\n",
    "        return \"No valid response received.\"\n",
    "\n",
    "# Exemple d'utilisation\n",
    "params = set_open_params()\n",
    "prompt = \"The sky is\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "formatted_response = format_openai_response(response)\n",
    "print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772cb9c-6706-41ce-bf14-827579322bd1",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec378e8-8dc0-4d4c-a144-32788d0d7e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Antibiotics are medications that treat bacterial infections by either killing the bacteria or preventing their reproduction, but they are ineffective against viral infections and misuse can lead to antibiotic resistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=1.0, max_tokens=2000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - temperature (float): Niveau de créativité dans les réponses.\n",
    "    - max_tokens (int): Nombre maximum de tokens dans la réponse.\n",
    "    - top_p (float): Contrôle la diversité via nucleus sampling.\n",
    "    - frequency_penalty (float): Pénalité pour la répétition de mots.\n",
    "    - presence_penalty (float): Pénalité pour l'introduction de nouveaux sujets.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire contenant les paramètres configurés.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=2000, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres avec une température de 0.7\n",
    "params = set_open_params(temperature=0.7)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a814f78-eaac-45b8-bf04-c4389b3bfd4a",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47461057-2796-439e-aa66-b6553f618554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Mice"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=1.0, max_tokens=2000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=2000, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
    "\n",
    "Question: What was OKT3 originally sourced from?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd7a55-6d2f-4c3d-81a5-a1e00af2e06f",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2abc0334-041d-4288-b6dd-d44816f8a2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Neutral"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=2000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=2000, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7)\n",
    "\n",
    "# Prompt pour classifier le texte\n",
    "prompt = \"\"\"Classify the text into neutral, negative, or positive.\n",
    "\n",
    "Text: I think the food was okay.\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839a0e6-9718-4c08-b14e-0036ade2f91b",
   "metadata": {},
   "source": [
    "##  Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47164eb7-b470-42bb-b19f-fed455199401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Black holes are created when massive stars exhaust their nuclear fuel and collapse under their own gravity. This collapse causes the star's core to shrink rapidly, forming a singularity with infinite density at the center. The intense gravitational pull of the singularity creates a region of spacetime from which nothing, not even light, can escape, known as the event horizon. This marks the formation of a black hole."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=2000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=2000, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7)\n",
    "\n",
    "# Prompt pour la conversation\n",
    "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant's tone is technical and scientific.\n",
    "\n",
    "Human: Hello, who are you?\n",
    "AI: Greetings! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the creation of black holes?\n",
    "AI:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd440d68-f6a4-4c0d-8a9e-0d4b51a41535",
   "metadata": {},
   "source": [
    "## Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914334c0-20e0-49b1-bfce-c4c446d2ec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SELECT StudentId, StudentName\n",
       "FROM students\n",
       "WHERE DepartmentId = (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=150, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7, max_tokens=100)\n",
    "\n",
    "# Prompt pour la requête MySQL\n",
    "prompt = \"\"\"\\\"\\\"\\\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "Create a MySQL query for all students in the Computer Science Department\n",
    "\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f01738-20a6-49f9-8904-bd2a585d53de",
   "metadata": {},
   "source": [
    "## Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc199e6-41b3-45b8-8991-c5524c40c848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Odd numbers: 15, 5, 13, 7, 1\n",
       "\n",
       "Adding them up: 15 + 5 + 13 + 7 + 1 = 41\n",
       "\n",
       "The result, 41, is an odd number."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=150, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7, max_tokens=150)\n",
    "\n",
    "# Prompt pour la tâche\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb185b-3f2e-4e20-960b-378a0b9fab82",
   "metadata": {},
   "source": [
    "## Few-shot prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bcbd039-bf66-4f16-8b46-16fc82b3f2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The answer is True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=150, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7, max_tokens=150)\n",
    "\n",
    "# Prompt pour l'analyse des nombres impairs\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae137025-3116-4558-bb40-71e4b1ffd8b2",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31ee5c8f-04fe-4e08-a90c-013e333e82e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=150, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7, max_tokens=150)\n",
    "\n",
    "# Prompt pour l'analyse des nombres impairs\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75798153-07db-4b7e-9c39-81c0b129acb8",
   "metadata": {},
   "source": [
    "## Zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9bb5434-768f-4e4f-9770-1364f94120ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "You started with 10 apples. \n",
       "You gave 2 to the neighbor and 2 to the repairman, so you have 10 - 2 - 2 = 6 apples left. \n",
       "You then bought 5 more apples, so you now have 6 + 5 = 11 apples. \n",
       "After eating 1 apple, you are left with 11 - 1 = 10 apples. \n",
       "\n",
       "So, you remained with 10 apples in the end."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv()\n",
    "# Obtenir la clé API depuis les variables d'environnement\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def set_open_params(temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Configure les paramètres par défaut pour l'appel à l'API OpenAI.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "\n",
    "def generate_openai_text(api_key, model, messages, temperature=1.0, max_tokens=150, top_p=1.0,\n",
    "                         frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Génère du texte en utilisant l'API OpenAI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Définir les paramètres\n",
    "params = set_open_params(temperature=0.7, max_tokens=150)\n",
    "\n",
    "# Prompt pour le problème d'apples\n",
    "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "# Créer les messages pour l'API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Appeler la fonction pour obtenir la réponse du modèle\n",
    "response = generate_openai_text(\n",
    "    api_key,\n",
    "    params[\"model\"],\n",
    "    messages,\n",
    "    temperature=params[\"temperature\"],\n",
    "    max_tokens=params[\"max_tokens\"],\n",
    "    top_p=params[\"top_p\"],\n",
    "    frequency_penalty=params[\"frequency_penalty\"],\n",
    "    presence_penalty=params[\"presence_penalty\"]\n",
    ")\n",
    "\n",
    "# Formater et afficher la réponse\n",
    "if response and \"choices\" in response:\n",
    "    display(Markdown(response['choices'][0]['message']['content']))\n",
    "else:\n",
    "    print(\"No valid response received.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
